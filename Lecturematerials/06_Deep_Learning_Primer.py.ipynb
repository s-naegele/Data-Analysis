{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ee76362ea25b819af56e1a6c0452717e",
     "grade": false,
     "grade_id": "cell-eda22d77650ef361",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning Primer\n",
    "![DL](https://www.ionos.de/digitalguide/fileadmin/DigitalGuide/Teaser/machine-learning-t.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "703d56ed56d8a93a54b41bc2022163c4",
     "grade": false,
     "grade_id": "cell-95fbee5cebad4462",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## From the Perceptron to a Dense Layer\n",
    "![foo](https://pythonmachinelearning.pro/wp-content/uploads/2017/09/Single-Perceptron.png.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "89130872bc403a6ae0e1a91f132b17ef",
     "grade": false,
     "grade_id": "cell-71235814b11c4709",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Neuron pre-activation (or input activation):\n",
    "\n",
    "$$ z(\\textbf{x}) = b + \\sum_i w_ix_i = b + \\mathbf{w}^T\\mathbf{x}$$\n",
    "\n",
    "Neuron (output) activation:\n",
    "$$a(z) = \\sigma(z(\\textbf{x})) = \\sigma(b + \\sum_i w_ix_i )$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "W = np.array([-1, 2])\n",
    "X = np.array([2,3])\n",
    "\n",
    "z = np.dot(X,W) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d6e225eb0b4e4bc220dafe0b7e5f9b6",
     "grade": false,
     "grade_id": "cell-129c0aed9139a84f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Non-Linearity\n",
    "\n",
    "Binary Step: $$sigma(z) = \\begin{cases}\n",
    "1,  & \\text{if $z$ > 0} \\\\\n",
    "0, & \\text{else}\n",
    "\\end{cases}$$\n",
    "Sigmoid: $$sigma(z) = \\frac{1}{1+e^{-z}}$$\n",
    "tanh: $$sigma(z) = tanh(z)$$\n",
    "\n",
    "ReLU: $$sigma(z) =\n",
    "\\begin{cases}\n",
    "z,  & \\text{if $z$ > 0} \\\\\n",
    "0, & \\text{else}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = np.arange(-8, 8, 0.1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(16,2),sharex=True)\n",
    "\n",
    "ax[0].plot(x,np.trunc(np.clip(x+0.99,0,1)))\n",
    "ax[0].grid()\n",
    "ax[0].set_title(\"Binary Step\")\n",
    "\n",
    "ax[1].plot(x,np.tanh(x))\n",
    "ax[1].grid()\n",
    "ax[1].set_title(\"tanh\")\n",
    "\n",
    "ax[2].plot(x,(1/(1+np.exp(-x))))\n",
    "\n",
    "ax[2].grid()\n",
    "ax[2].set_title(\"sigmoid\")\n",
    "\n",
    "ax[3].plot(x,np.maximum(0.,x))\n",
    "ax[3].grid()\n",
    "ax[3].set_title(\"ReLU\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### One Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "# Plot the surface.\n",
    "fig = plt.figure(figsize=(16,9))\n",
    "\n",
    "def make_image_surface_plot(ax, X, Z):\n",
    "    surf = ax.plot_surface(X[0], X[1], Z, cmap=cm.coolwarm,\n",
    "                           linewidth=0, antialiased=False)\n",
    "    # Customize the z axis.\n",
    "    ax.set_zlim(-1.01, 1.01)\n",
    "    ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "    ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "    ax.set_xlabel(\"$X_1$\")\n",
    "    ax.set_ylabel(\"$X_2$\")\n",
    "    ax.set_zlabel(\"$Z$\")\n",
    "\n",
    "    # Add a color bar which maps values to colors.\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "\n",
    "X = np.mgrid[-5:5:0.25, -5:5:0.25]\n",
    "    \n",
    "# Make data.\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "W1 = np.array([-1, 2])\n",
    "Z1 = np.tanh(X[0] * W1[0] + X[1] * W1[1])\n",
    "make_image_surface_plot(ax, X, Z1)\n",
    "\n",
    "# Make data.\n",
    "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "W2 = np.array([0.6, 0.1])\n",
    "Z2 = np.tanh(X[0] * W2[0] + X[1] * W2[1])\n",
    "make_image_surface_plot(ax, X, Z2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c3ca3c58bda00fffd2f113848923bb1",
     "grade": false,
     "grade_id": "cell-b00ee14a9f033cb8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Going Deeper\n",
    "![Deep](https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_01_03-DeepNeuralNetwork-WHITEBG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6ddcb93dd7260689f2af86e94de6245",
     "grade": false,
     "grade_id": "cell-100c2d309b7de80a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How to train a perceptron\n",
    "Make a prediction with weights\n",
    "\n",
    "Sources: \n",
    "- https://www.engineerknow.com/2021/12/gradient-descent-with-interactive.html?m=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "742e9a41a742e59ba3277a371f1a8938",
     "grade": false,
     "grade_id": "cell-fbc7fe021989c996",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interactive\n",
    "\n",
    "x_poly = np.linspace(-3,5,81)\n",
    "x_start = -3\n",
    "\n",
    "def cost_function(X):\n",
    "    return 2*X**2-4*X\n",
    "\n",
    "def gradient(X):\n",
    "    return  (4*X) -4\n",
    "\n",
    "y_poly = cost_function(x_poly)\n",
    "\n",
    "def f(iterations,learning_rate):\n",
    "    x_path = np.empty(iterations,)\n",
    "    x_path[0] = x_start\n",
    "    for i in range (1,iterations):\n",
    "        derivative =gradient(x_path[i-1])\n",
    "        x_path[i] = x_path[i-1]-(derivative*learning_rate)\n",
    "    x_path\n",
    "    plt.plot(x_poly,y_poly)\n",
    "    plt.plot(x_path,cost_function(x_path),'-o')\n",
    "\n",
    "interactive_plot = interactive(f,iterations = (1,20),learning_rate= (0.01,1,.1))\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf8cff58ab7cfc67fa7c7f6b937f7ef9",
     "grade": false,
     "grade_id": "cell-dce17a200c02181a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Stochastic Gradient Decent\n",
    "\n",
    "No math - let's save that for the masters course!\n",
    "\n",
    "Source: \n",
    "- https://machinelearningmastery.com/implement-perceptron-algorithm-scratch-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def predict(row, weights):\n",
    "    activation = weights[0]   # bias\n",
    "    for i in range(len(row)-1):\n",
    "        activation += weights[i + 1] * row[i]\n",
    "    return 1.0 if activation >= 0.0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f010f9a1e5a251663125b1a324968cdc",
     "grade": false,
     "grade_id": "cell-0cfb0359dcecb1e9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Estimate Perceptron weights using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def train_weights(train, l_rate, n_epoch):\n",
    "    # initialize with random float. In this case we are doing it with 0 to make it reproducible\n",
    "    weights = [0.0 for i in range(len(train[0]))]\n",
    "\n",
    "    # here we are iterating ``n_epoch`` times over the entire dataset\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0.0\n",
    "        for row in train:\n",
    "            prediction = predict(row, weights)\n",
    "            error = row[-1] - prediction\n",
    "            sum_error += error**2\n",
    "            weights[0] = weights[0] + l_rate * error\n",
    "            for i in range(len(row)-1):\n",
    "                weights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ef223added0fd38067f3928b10f167a",
     "grade": false,
     "grade_id": "cell-baeaa1d39b9bc63e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Learning the data\n",
    "Calculate weights, based on given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dataset = np.array([[2.7810836, 2.550537003, 0],\n",
    "    [1.465489372, 2.362125076, 0],\n",
    "    [3.396561688, 4.400293529, 0],\n",
    "    [1.38807019, 1.850220317, 0],\n",
    "    [3.06407232, 3.005305973, 0],\n",
    "    [7.627531214, 2.759262235, 1],\n",
    "    [5.332441248, 2.088626775, 1],\n",
    "    [6.922596716, 1.77106367, 1],\n",
    "    [8.675418651, -0.242068655, 1],\n",
    "    [7.673756466, 3.508563011, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "l_rate = 0.1\n",
    "n_epoch = 5\n",
    "W = train_weights(dataset, l_rate, n_epoch)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualize Result\n",
    "\n",
    "Let's see, how this looks like in 3D-Space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = np.mgrid[0:10:0.25, -5:5:0.25]\n",
    "Z = X[0] * W[1] + X[1] * W[2] + W[0]\n",
    "Z[Z<0] = 0\n",
    "Z[Z>0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(x=dataset[:,0], y=dataset[:,1], z=dataset[:,2], \n",
    "                                   mode='markers', marker=dict(size=3)),\n",
    "                     go.Surface(z=Z, x=X[0], y=X[1], opacity=0.2)])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "96084f7050a18aa888a3e0105f280e3e",
     "grade": false,
     "grade_id": "cell-b9f48961357f5a01",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Further Information:\n",
    "- [Hugo Larochelle (Google Brain) on the NN Theorie](https://www.youtube.com/watch?v=O2o4oONWCWA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "711cf87153e821955979481bdfb68ad8",
     "grade": false,
     "grade_id": "cell-9adc50ebb6b43ed6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional Neural Networks: Simple MNIST convnet\n",
    "\n",
    "The following simple example ist taken from the Keras Examples by [fchollet](https://twitter.com/fchollet)\n",
    "as the orginal author.\n",
    "\n",
    "The following codetakes the MNIST image dataset as input (images of handwritten images) and performs\n",
    "a classification on those. The applied simple convnet achieves ~99% test accuracy.\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pip\n",
    "!pip install tensorflow==2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "db327994aff1c44a32db1b1d78e7db19",
     "grade": false,
     "grade_id": "cell-6914dace89ce6040",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Prepare the data\n",
    "Model / data parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6bc0b91133fc4e89928dac1792028ecc",
     "grade": false,
     "grade_id": "cell-03fec1f75ee656d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If not yet done, the data will be downloaded. Then, it is imported. Additionally a split between train and test sets is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "num_images, _, _ = x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4459e6456348dfe0daa1e37f88b7943b",
     "grade": false,
     "grade_id": "cell-0a4ecdc749062fa0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Let's have a look at the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,5, figsize=(16,8))\n",
    "for img_dx in range(5):    \n",
    "    idx = np.random.randint(num_images)\n",
    "    ax[img_dx].imshow(x_train[idx], cmap=\"gray\")\n",
    "    ax[img_dx].set_title(y_train[idx])\n",
    "    ax[img_dx].set_yticklabels([])\n",
    "    ax[img_dx].set_xticklabels([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b5c2b6b771ade7c321e4e384053bb0c2",
     "grade": false,
     "grade_id": "cell-c373cdb377683d91",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Preprocessing\n",
    "Scale images to the range of $[0, 1]$. Wer are inputing floats to out model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dbe2392256e693239a908298e449e7fd",
     "grade": false,
     "grade_id": "cell-db42aae2d7aa8b53",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Make sure images have shape (28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "955d57a8637ba7fe99af2f91fd0019ce",
     "grade": false,
     "grade_id": "cell-dd0384de65539780",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " #### Labels\n",
    " How does the `y` data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for _ in range(10):    \n",
    "    idx = np.random.randint(num_images)\n",
    "    print(y_train[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "56da46884152694a558d09988a3eb2e7",
     "grade": false,
     "grade_id": "cell-30fb5bc188f8d5c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, we need convert the  class vectors to binary class matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for _ in range(10):    \n",
    "    idx = np.random.randint(num_images)\n",
    "    print(y_train[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d88eca0cec3ccf500ec1da5935de3a73",
     "grade": false,
     "grade_id": "cell-78708424e1ea7538",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Convolutional Neural Networks (CNNs)\n",
    "![\"D Discrete Convolution](https://miro.medium.com/max/535/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ca1a1e39238a0e643071584823a7df6",
     "grade": false,
     "grade_id": "cell-57e97877ed14a650",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Build our Model\n",
    "![Architecture](https://davidstutz.de/wordpress/wp-content/uploads/2019/10/cnn.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import time\n",
    "\n",
    "class LogCallback(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.acc = []\n",
    "        self.val_acc = []\n",
    "        self.fig = plt.figure()\n",
    "        self.logs = []\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={None}):\n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.i += 1\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.acc.append(logs.get('accuracy'))\n",
    "        self.val_acc.append(logs.get('val_accuracy'))\n",
    "\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, sharex=True, figsize=(16,9))\n",
    "\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        ax1.plot(self.x, self.losses, label=\"training loss\")\n",
    "        ax1.plot(self.x, self.val_losses, label=\"validation loss\")\n",
    "        ax1.set_title(\"Loss\")\n",
    "        ax1.set_xlabel(\"Num Epochs\")\n",
    "        ax1.set_ylabel(\"Loss\")\n",
    "        ax1.grid()\n",
    "        ax1.legend()\n",
    "\n",
    "        ax2.plot(self.x, self.acc, label=\"training accuracy\")\n",
    "        ax2.plot(self.x, self.val_acc, label=\"validation accuracy\")\n",
    "        ax2.set_title(\"Accuracy\")\n",
    "        ax1.set_xlabel(\"Num Epochs\")\n",
    "        ax1.set_ylabel(\"Accuracy [%]\")\n",
    "        ax2.grid()\n",
    "        ax2.legend()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        print(f'Total training time: {time.time()-self.start_time:.2f} sec.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "db59791281073647b32b497673cc43c4",
     "grade": false,
     "grade_id": "cell-ff49550eda4e86ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training the model\n",
    "\n",
    "Here, we are using a `batch_size` of $128$ and $15$ epochs for training the network. $10\\%$ of the data is used for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, batch_size=128, epochs=15, validation_split=0.1, callbacks=[LogCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b09cdbba1df190c79ab3202ea0dff7d0",
     "grade": false,
     "grade_id": "cell-9d410474db092fd9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4a480d3841608946972c0fca96aade7a",
     "grade": false,
     "grade_id": "cell-4a08a776fc3ade23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent Neural Networks (RNNs): Character-Level Trump Tweet Generator\n",
    "This notebook is taken from the current course Deep Learning, 4th semester masters. For the application of textual data, we are building a language model.\n",
    "This sample code demonstrates how to train a stacked LSTM on a set of Tweets and then to generate text that resembles these input Tweets in style. We work with Trump Tweets as collected by the [Trump Twitter Archive](http://www.trumptwitterarchive.com/archive). This form of learning doesn't need manually annotated datasets.\n",
    "\n",
    "\n",
    "But first..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bbd77265b343d81e00e7abe9a0dc4011",
     "grade": false,
     "grade_id": "cell-1078e2e04b628c85",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why RNNs?\n",
    "![RNNs](http://karpathy.github.io/assets/rnn/diags.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c910395f68a298f8a661ce9068d7aa3",
     "grade": false,
     "grade_id": "cell-ad5b06ef4b24f487",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![unrolling](https://machinelearningmastery.com/wp-content/uploads/2017/08/Example-of-an-RNN-with-a-cycle.png)\n",
    "![unrolling](https://machinelearningmastery.com/wp-content/uploads/2017/08/Example-of-Unrolled-RNN-on-the-forward-pass.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "37c2411ae140be831f8aca274fab8990",
     "grade": false,
     "grade_id": "cell-fe8a5e4a7d8bf3b9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![RNNs](http://karpathy.github.io/assets/rnn/charseq.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "105fc23750ae6a6bbb0dcb7babfdaf9a",
     "grade": false,
     "grade_id": "cell-ce569471b0f32617",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training\n",
    "The following part demonstrates the training of such an LSTM model.\n",
    "\n",
    "#### Parameters\n",
    "Some parameters to set and tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "SEQUENCE_LEN = 60\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 15\n",
    "HIDDEN_LAYERS_DIM = 512\n",
    "LAYER_COUNT = 4\n",
    "DROPOUT = 0.2\n",
    "\n",
    "data_directory = \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f44e09b67bd22da79c7a26260c775ea",
     "grade": false,
     "grade_id": "cell-8808e451c41dc3f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's create the vocabulary! Dependend on the dataset and your set locale, it mightl be possible, that you also need to remove other chars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "characters = list(string.printable)\n",
    "characters.remove('\\x0b')\n",
    "characters.remove('\\x0c')\n",
    "\n",
    "VOCABULARY_SIZE = len(characters)\n",
    "\n",
    "characters_to_idx = {c:i for i,c in enumerate(characters)}\n",
    "print(f'vocabulary len = {VOCABULARY_SIZE} with the following characters: {characters}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d01d9a2d9912eb8afce798c72147a026",
     "grade": false,
     "grade_id": "cell-b618787de8e5c1d5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here are two helper functions. Take them as they are. :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def batch_generator(text, count):\n",
    "    \"\"\"Generate batches for training\"\"\"\n",
    "    while True: # keras wants that for ... reasons\n",
    "        for batch_idx in range(count):\n",
    "            X = np.zeros((BATCH_SIZE, SEQUENCE_LEN, VOCABULARY_SIZE))\n",
    "            y = np.zeros((BATCH_SIZE, VOCABULARY_SIZE))\n",
    "\n",
    "            batch_offset = BATCH_SIZE * batch_idx\n",
    "\n",
    "            for sample_idx in range(BATCH_SIZE):\n",
    "                sample_start = batch_offset + sample_idx\n",
    "                for s in range(SEQUENCE_LEN):\n",
    "                    X[sample_idx, s, characters_to_idx[text[sample_start+s]]] = 1\n",
    "                y[sample_idx, characters_to_idx[text[sample_start+s+1]]]=1\n",
    "\n",
    "            yield X, y\n",
    "\n",
    "\n",
    "def describe_batch(X, y, samples=3):\n",
    "    \"\"\"Describe in a human-readable format some samples from a batch\"\"\"\n",
    "    for i in range(samples):\n",
    "        sentence = \"\"\n",
    "        for s in range(SEQUENCE_LEN):\n",
    "            sentence += characters[X[i,s,:].argmax()]\n",
    "        next_char = characters[y[i,:].argmax()]\n",
    "\n",
    "        print(\"sample #%d: ...%s -> '%s'\" % (\n",
    "            i,\n",
    "            sentence[-20:],\n",
    "            next_char\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e572f3b87c34173125c71a0b8ed4735",
     "grade": false,
     "grade_id": "cell-b386c7a10e839470",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here, the text is loaded, (its size reduced,) and described initially. How does it look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# loading the text\n",
    "with open(os.path.join(data_directory, \"trump_train.txt\"), \"r\", encoding=\"utf8\") as f:\n",
    "    text_train = f.read()\n",
    "with open(os.path.join(data_directory, \"trump_val.txt\"), \"r\", encoding=\"utf8\") as f:\n",
    "    text_val = f.read()\n",
    "\n",
    "text_train_len = len(text_train)\n",
    "text_val_len = len(text_val)\n",
    "print(\"Total of %d characters\" % (text_train_len + text_val_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66dc1f35e6e85808811212ea89b39967",
     "grade": false,
     "grade_id": "cell-903e1715e51de436",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's see, how the `batch_generator` prepares the data. Try out the generator and describe one batch using the `describe_batch(...)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for ix, (X,y) in enumerate(batch_generator(text_train, count=1)):\n",
    "    # describe some samples from the first batch\n",
    "    describe_batch(X, y, samples=5)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a52dcfbfa1bdce0e64c0e46fa671adfc",
     "grade": false,
     "grade_id": "cell-73915fe1df140e93",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's create a model an have a look at its summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "training_model = keras.Sequential()\n",
    "for layer_idx in range(LAYER_COUNT):\n",
    "    training_model.add(\n",
    "        layers.LSTM(\n",
    "            HIDDEN_LAYERS_DIM,\n",
    "            return_sequences=True if (layer_idx!=(LAYER_COUNT-1)) else False,\n",
    "            input_shape=(SEQUENCE_LEN, VOCABULARY_SIZE),\n",
    "        )\n",
    "    )\n",
    "    training_model.add(layers.Dropout(DROPOUT))\n",
    "\n",
    "training_model.add(layers.Dense(VOCABULARY_SIZE, activation='softmax'))\n",
    "\n",
    "training_model.compile(loss='categorical_crossentropy', optimizer=\"adam\")\n",
    "training_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10b6bdeaf651997e880dc4faf9049e7d",
     "grade": false,
     "grade_id": "cell-c1d423ca2f661a62",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Callbacks\n",
    "Both are passed to the training proceedure. \n",
    "\n",
    "- The first one always creates a checkpoint of a model, if it achieves a new best validation score\n",
    "- the second checks, if the model gets into overfitting and stops early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "filepath = \"./BS-%d_%d-%s_dp%.2f_%dS_epoch{epoch:02d}-loss{loss:.4f}-val-loss{val_loss:.4f}_weights\" % (\n",
    "    BATCH_SIZE,\n",
    "    LAYER_COUNT,\n",
    "    HIDDEN_LAYERS_DIM,\n",
    "    DROPOUT,\n",
    "    SEQUENCE_LEN\n",
    ")\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath,\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "callbacks_list = [checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e8c8a255b14fc582a7a8ed8ddbfe46ad",
     "grade": false,
     "grade_id": "cell-06622bd38fc42fa9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Training procedure\n",
    "\n",
    "Let's get the trianing started!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_batch_count = (text_train_len - SEQUENCE_LEN) // BATCH_SIZE\n",
    "val_batch_count = (text_val_len - SEQUENCE_LEN) // BATCH_SIZE\n",
    "\n",
    "history = training_model.fit(\n",
    "    batch_generator(text_train, count=train_batch_count),\n",
    "    steps_per_epoch=train_batch_count,\n",
    "    max_queue_size=4, # no more than one queued batch in RAM\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks_list,\n",
    "    validation_data=batch_generator(text_val, count=val_batch_count),\n",
    "    validation_steps=val_batch_count,\n",
    "    initial_epoch=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "831e53110460204e4f20cc2436450e64",
     "grade": false,
     "grade_id": "cell-96bb3c91296d9e8a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Testing\n",
    "\n",
    "The following part demonstrates the testing of the model - in other words, how to generate text that sounds like Trump. To keep the prediction step simple, a batch size of 1 is used. In tensorflow v1, the batch size cannot be changed once the model is build. This means that we have to rebuild the model and restore the weights from the training checkpoint.\n",
    "\n",
    "Here, we are building up a new model and loading the weights to the model.\n",
    "\n",
    "Let's start generating text...\n",
    "\n",
    "\n",
    "In the following, create a model, similar as done in the training section.  This time, the model is built up similar, but without componentes, that are only usefull during training (e.g. dropout). Additionally, instead of the `input_shape`, `batch_input_shape` is used, so that only one word is passed along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_model = keras.Sequential()\n",
    "for i in range(LAYER_COUNT):\n",
    "    test_model.add(\n",
    "            layers.LSTM(\n",
    "                HIDDEN_LAYERS_DIM,\n",
    "                return_sequences=(i!=(LAYER_COUNT-1)),\n",
    "                batch_input_shape=(1, 1, VOCABULARY_SIZE),\n",
    "                stateful=True\n",
    "            )\n",
    "        )\n",
    "test_model.add(layers.Dense(VOCABULARY_SIZE, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b366f17f9ba6105d725ed90a7796fb3",
     "grade": false,
     "grade_id": "cell-13eade2085085b8e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Restoring checkpoint \n",
    "Load the weights with the best validation results during the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_model.load_weights(\"weights_4-512-lstm_loss1.2550_val-loss1.2443\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a872910896154181d7490ffdc6e6d471",
     "grade": false,
     "grade_id": "cell-77afbb03a6ae1384",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Sampling based on Softmax using Temperature and Randomness\n",
    "\n",
    "A activation function, that was not discussed yet, is the so called _Softmax_ [1]:\n",
    "$$q_i = \\frac{e^{z_i / T}}{\\sum_j e^{z_j / T}}$$ where $T$ can be set to any value in $[0,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    \"\"\"Helper function to sample an index from a probability array\"\"\"\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds)\n",
    "\n",
    "    preds = preds / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds/np.sum(preds), 1)\n",
    "\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "adc367d91ebb082b5c348f0527e2becd",
     "grade": false,
     "grade_id": "cell-7eec7eab75c66826",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can consider these two function as they are.\n",
    "\n",
    "`predict_next_char` predicts a probability distribution for a following character, based on an input. This probability distribution is then used to sample character based on this distribution.\n",
    "\n",
    "The `generate_text`functions generates a text using a model, based on the `seed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def predict_next_char(model, current_char, diversity=1.0):\n",
    "    \"\"\"Predict the next character from the current one\"\"\"\n",
    "    x = np.zeros((1, 1, VOCABULARY_SIZE))\n",
    "    x[:,:,characters_to_idx[current_char]] = 1\n",
    "    y = model.predict(x, batch_size=1)\n",
    "    next_char_idx = sample(y[0,:], temperature=diversity)\n",
    "    next_char = characters[next_char_idx]\n",
    "    return next_char\n",
    "\n",
    "def generate_text(model, seed=\"I am\", count=140):\n",
    "    \"\"\"Generate characters from a given seed\"\"\"\n",
    "    model.reset_states()\n",
    "    for s in seed[:-1]:\n",
    "        next_char = predict_next_char(model, s)\n",
    "    current_char = seed[-1]\n",
    "\n",
    "    sys.stdout.write(\"[\"+seed+\"]\")\n",
    "\n",
    "    # no more reset, preserve context\n",
    "    for i in range(count - len(seed)):\n",
    "        next_char = predict_next_char(model, current_char, diversity=0.7)\n",
    "        current_char = next_char\n",
    "        sys.stdout.write(next_char)\n",
    "    print(\"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b853031790e9dd5860239b7836cec6ba",
     "grade": false,
     "grade_id": "cell-53d05677b28e686b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Let's beginn the fun!\n",
    "\n",
    "Now, lets write some tweets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    generate_text(\n",
    "        test_model,\n",
    "        seed=\"Despite the constant negative press \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9469694d18614c5448a0a0d856382a8a",
     "grade": false,
     "grade_id": "cell-daa9941cc2578ad4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Try it with your own seed sentence.\n",
    "\n",
    "The question that unanswered till now: what did he want to tell us?\n",
    "\n",
    "[![covefe](https://pbs.twimg.com/media/DBIXi67V0AAzS7x?format=jpg&name=900x900)](https://twitter.com/GavinNewsom/status/869783572390883328/photo/1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "rise": {
   "enable_chalkboard": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
